{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The objective of this report is to register the overall steps of every model trained during the challenge of Fraud Detection offer by the Tera and NVidia. Models All pickle files with the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Home"},{"location":"#models","text":"All pickle files with the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Models"},{"location":"baseline/","text":"These was the steps of my baseline using Logistic Regression and very simple transformations. The data was not balanced by any technique and the model was not tunned as well. File name: lrc_baseline.sav Pipeline graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics]; Confusion Matrix","title":"Baseline"},{"location":"baseline/#pipeline","text":"graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics];","title":"Pipeline"},{"location":"baseline/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"metrics/","text":"After the training and prediction steps, all the metrics will be collected and added on this report. Perfomance Table Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time to Run baseline 0.964 0.082 0.997 0.541 0.153 26.77 s model-v1 0.843 0.446 0.999 0.722","title":"Metrics"},{"location":"metrics/#perfomance-table","text":"Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time to Run baseline 0.964 0.082 0.997 0.541 0.153 26.77 s model-v1 0.843 0.446 0.999 0.722","title":"Perfomance Table"},{"location":"model_v1/","text":"These was the steps of the model-v1 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. The data was not balanced by any technique and the model was not tunned as well. File name: 19_01_22_lr_v1.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics];","title":"Model-v1"},{"location":"model_v1/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics];","title":"Pipeline"}]}