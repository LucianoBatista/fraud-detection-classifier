{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The objective of this documentation is to provide an overview about the project. Here, you'll find all the steps of a Data Science Project: Exploratory Data Analysis Iterative Modeling Metrics Analysis API Development Deploy on Heroku The motivation of this project came from the Fraud Detection Challenge offer by the Tera and NVidia. The Challenge Fraud is a problem that impact a lot of different fields, like: government, bank, ratailer, micro-entrepreneur. So, is very important to everyone to know how to face this challenge. The ACFE report ( Association of Certified Fraud Examiners ) corroborates this perception by pointing to a forecast of 60% growth in the next two years in investments in anti-fraud . In the same report, 58% of companies declare that: ... we do not have sufficient levels, resources and professionals to act in anti-fraud actions. According to Psafe , from January to August of last year there were 920 thousand cases in only in Brazil, meaning that 3.6 frauds happen in the country per minute. For example, more than 11 million bank phishing attempts were detected. In the November 2020 report is listed among the financial sector's priorities to fraud reduction and the use of an artificial intelligence system to monitor it . According to Febraban ( Brazilian Federation of Banks ) to mitigate this risk, an annual expenditure of around BRL 2 billion on IT per year is expected in Brazil. However, the risk is heightened not only by the sophistication of cyber criminals, but also by consumer dissatisfaction that not only abandons the customer base, but also uses the consumer code itself, which guarantees double compensation of the amounts charged. In this scenario, artificial intelligence emerges as a tool that gives more robustness, agility and flexibility to combat fraud , working 24 hours a day, 7 days a week, making it a more than possible way, necessary for financial institutions to can effectively combat fraudsters. An example of this is American Express, with more than 115 million active credit cards, which fights fraud using inference and Deep Learning, reducing fraud-related expenses by US$ 1.2 trillion , in detection processes that happen in milliseconds . This fact guarantees the company the lowest fraud rate in the world for 13 consecutive years. The Mission You , as a manager, have the mission in this challenge to dethrone American Express as the best institution in the fight against fraud. To do so, you need to propose a solution for fraud detection and analysis that can reduce the company's risks and ensure healthy margins. The mission is not easy, as the large volume of information and technological legacy are the reality of most corporations in the sector, even among leading companies. In addition, artificial intelligence initiatives demand great capacity Models All models are serialized as pickle files to reproducibility. All the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Home"},{"location":"#the-challenge","text":"Fraud is a problem that impact a lot of different fields, like: government, bank, ratailer, micro-entrepreneur. So, is very important to everyone to know how to face this challenge. The ACFE report ( Association of Certified Fraud Examiners ) corroborates this perception by pointing to a forecast of 60% growth in the next two years in investments in anti-fraud . In the same report, 58% of companies declare that: ... we do not have sufficient levels, resources and professionals to act in anti-fraud actions. According to Psafe , from January to August of last year there were 920 thousand cases in only in Brazil, meaning that 3.6 frauds happen in the country per minute. For example, more than 11 million bank phishing attempts were detected. In the November 2020 report is listed among the financial sector's priorities to fraud reduction and the use of an artificial intelligence system to monitor it . According to Febraban ( Brazilian Federation of Banks ) to mitigate this risk, an annual expenditure of around BRL 2 billion on IT per year is expected in Brazil. However, the risk is heightened not only by the sophistication of cyber criminals, but also by consumer dissatisfaction that not only abandons the customer base, but also uses the consumer code itself, which guarantees double compensation of the amounts charged. In this scenario, artificial intelligence emerges as a tool that gives more robustness, agility and flexibility to combat fraud , working 24 hours a day, 7 days a week, making it a more than possible way, necessary for financial institutions to can effectively combat fraudsters. An example of this is American Express, with more than 115 million active credit cards, which fights fraud using inference and Deep Learning, reducing fraud-related expenses by US$ 1.2 trillion , in detection processes that happen in milliseconds . This fact guarantees the company the lowest fraud rate in the world for 13 consecutive years.","title":"The Challenge"},{"location":"#the-mission","text":"You , as a manager, have the mission in this challenge to dethrone American Express as the best institution in the fight against fraud. To do so, you need to propose a solution for fraud detection and analysis that can reduce the company's risks and ensure healthy margins. The mission is not easy, as the large volume of information and technological legacy are the reality of most corporations in the sector, even among leading companies. In addition, artificial intelligence initiatives demand great capacity","title":"The Mission"},{"location":"#models","text":"All models are serialized as pickle files to reproducibility. All the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Models"},{"location":"api/","text":"","title":"API"},{"location":"baseline/","text":"These was the steps of my baseline using Logistic Regression and very simple transformations. The data was not balanced by any technique and the model was not tunned as well. File name: lrc_baseline.sav Pipeline graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics]; Confusion Matrix","title":"Baseline"},{"location":"baseline/#pipeline","text":"graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics];","title":"Pipeline"},{"location":"baseline/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"eda/","text":"Fraud Detection EDA A fraudulent action here is the act of invade some user bank account, and empty it by transferring it to another account and them withdrawing the money. Libraries import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl import pingouin as pg import math import warnings from src.plotting.plotting import bar_plot , ecdf_plot , boxplot_plot # supressing warnings warnings . filterwarnings ( \"ignore\" ) mpl . rcParams [ \"figure.dpi\" ] = 120 sns . set_style ( \"whitegrid\" ) sns . set_palette ( \"deep\" ) Our Dataset # data fraud_detect_df = pd . read_csv ( \"data/fraud_detection_dataset.csv\" ) fraud_detect_df . columns = [ \"step\" , \"type\" , \"amount\" , \"name_orig\" , \"old_balance_orig\" , \"new_balance_orig\" , \"name_dest\" , \"old_balance_dest\" , \"new_balance_dest\" , \"is_fraud\" , \"is_flagged_fraud\" , ] # change the order, last columns with the target fraud_detect_df = fraud_detect_df [ [ \"step\" , \"type\" , \"amount\" , \"name_orig\" , \"old_balance_orig\" , \"new_balance_orig\" , \"name_dest\" , \"old_balance_dest\" , \"new_balance_dest\" , \"is_flagged_fraud\" , \"is_fraud\" , ] ] How unbalanced is our data? balance_df = fraud_detect_df [ \"is_fraud\" ] . value_counts ( normalize = True ) . reset_index () balance_df [ \"index\" ] = balance_df [ \"index\" ] . replace ({ 0 : \"not fraud\" , 1 : \"fraud\" }) balance_df [ \"is_fraud\" ] = round ( balance_df [ \"is_fraud\" ], 4 ) # plot bar_plot ( balance_df , x = \"is_fraud\" , y = \"index\" , xlabel = \"Proportion\" , ylabel = \"\" ) How is_fraud is distributed on our categorical variable type ? type_by_fraud_df = fraud_detect_df [[ \"type\" , \"is_fraud\" ]] . value_counts ( normalize = True ) . reset_index () type_by_fraud_df . columns = [ \"type\" , \"is_fraud\" , \"prop\" ] type_by_fraud_df [ \"is_fraud\" ] = type_by_fraud_df [ \"is_fraud\" ] . replace ({ 0 : \"not fraud\" , 1 : \"fraud\" }) type_by_fraud_df [ \"prop\" ] = round ( type_by_fraud_df [ \"prop\" ], 4 ) bar_plot ( type_by_fraud_df , x = \"prop\" , y = \"type\" , xlabel = \"Proportion\" , ylabel = \"\" , hue = \"is_fraud\" ) Let's perform a Chi2 test, and check if the correlation with our target ( is_fraud ) is significant (our alternative hypothesis). observed , expected , stats = pg . chi2_independence ( fraud_detect_df , x = \"type\" , y = \"is_fraud\" ) stats . round ( 4 ) As wee see, this is a nice variable to add to our model, very low p-values on different tests. What about the variable is_flagged_fraud ? is_ff_df = fraud_detect_df [ \"is_flagged_fraud\" ] . value_counts () . reset_index () is_ff_df . columns = [ \"is_fraud\" , \"count\" ] fraud_detect_df . drop ([ \"is_flagged_fraud\" ], axis = 1 , inplace = True ) is_fraud count 0 6.362.604 1 16 This variable is not representative, for now wi'll just drop it. Let's look at the amount variable We'll be using the ECDF plot here because the size of the dataset is too large, and because of the scale, is really difficult to see, in a meaningful way, the distribution. The ECDF plot will bring on y-axis the percentiles, and x-axis your values for the numeric variable. ecdf_plot ( data = fraud_detect_df , x = \"amount\" , xlabel = \"Amount\" , ylabel = \"Density\" , hue = \"is_fraud\" ) Both of the distributions are skewed to the left, and in 60% of the time will be very difficult to distinguish if is fraud or not, based on those values. So, lets bring those variables to a more normal distribution applying the log transformation. # applying log transformation on this variable def log_transform ( x ): log_transformed = np . log ( x + 1 ) return log_transformed fraud_detect_df [ \"amount_log\" ] = fraud_detect_df [ \"amount\" ] . apply ( log_transform ) fraud_detect_df # let's see for the amount after the transformation ecdf_plot ( data = fraud_detect_df , x = \"amount_log\" , xlabel = \"Amount\" , ylabel = \"Density\" , hue = \"is_fraud\" ) Now, in most of the cases we can clearly see that exist difference between fraud and not fraud based on log transformed values of the amount category. Observing those values as a boxplot, we can see the difference on the medians and infer that those distributions are really different. _ = sns . boxplot ( x = \"is_fraud\" , y = \"amount_log\" , data = fraud_detect_df ) plt . show () So, seems reasonable that we use amount_log for the the rest of the analysis. # droping amount variable fraud_detect_df . drop ([ \"amount\" ], axis = 1 , inplace = True ) fraud_detect_df # let's check this variable related to the type of the transaction boxplot_plot ( data = fraud_detect_df , x = \"type\" , y = \"amount_log\" , hue = \"is_fraud\" , xlabel = \"\" , ylabel = \"Amount (log scale)\" , ) In most of the cases we can see a clear distinction between type , amount and is_fraud . But, for TRANSFER class, the distributions are very similar. Maybe if we remove some lower outliers we could improve the distinction between those distributions, for now, we'll continue without handling outliers. How the hour of the transaction is related to is_fraud ? The step variable is the hour of the transaction . fraud_on_hours_summary_df = ( fraud_detect_df . groupby ([ \"step\" ])[ \"is_fraud\" ] . agg ( count_fraud = sum , mean_fraud = np . mean , median_fraud = np . median , count_values = np . size , ) . reset_index () ) # line plot fig_dims = ( 27 , 4 ) fig , ax = plt . subplots ( figsize = fig_dims ) _ = sns . lineplot ( x = \"step\" , y = \"mean_fraud\" , data = fraud_on_hours_summary_df , ax = ax ) plt . show () This pattern is interesting because seems that there is a threshold on 400hr region, where we have really different kind of distributions. So, I checked what is happing before and after this period regarding to is_fraud variable. is_fraud count before_400hr count after_400hr 0 5.782K 571K 1 4.4K 3.7K We're seeing that we have less data on the second block of the plot, especially for non-fraud. Otherwise, the quantity of fraud stay very similar on both blocks. So, the percentage of fraud on the first 400 hours is 0.0773% and on the second is 0.649% We can also create another variable regarding the day of the month that those ours represent, and perform our analyse on both variables. def get_day ( x : int ): division = x / 24 dom = math . ceil ( division ) return dom fraud_detect_df [ \"day_of_month\" ] = fraud_detect_df [ \"step\" ] . apply ( get_day ) Now, we'll see how this variable is related to is_fraud and the amount_log , but also we'll look at the boxplot for each of the classes on type that have fraud on the observation. df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"TRANSFER\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'TRANSFER'\" , horizontal = True ) df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"CASH_OUT\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'CASH_OUT'\" , horizontal = True ) Was observed that the amount variable , is related just to the difference between origin and not the destination. So, let's add this variable to our dataset. fraud_detect_df [ \"amount_dest\" ] = abs (( fraud_detect_df [ \"old_balance_dest\" ]) - ( fraud_detect_df [ \"new_balance_dest\" ])) fraud_detect_df [ \"amount_dest_log\" ] = fraud_detect_df [ \"amount_dest\" ] . apply ( log_transform ) And also, investigate the those boxplots. df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"TRANSFER\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_dest_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'TRANSFER'\" , horizontal = True ) df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"CASH_OUT\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'CASH_OUT'\" , horizontal = True ) ## Exporting our final dataset for modeling final_fraud_df = fraud_detect_df [[ \"day_of_month\" , \"type\" , \"amount_log\" , \"amount_dest_log\" , \"is_fraud\" ]] final_fraud_df . to_csv ( \"data/second-eda-output.csv\" , index = False ) The Final Dataset","title":"Exploratory Data Analysis"},{"location":"eda/#fraud-detection-eda","text":"A fraudulent action here is the act of invade some user bank account, and empty it by transferring it to another account and them withdrawing the money.","title":"Fraud Detection EDA"},{"location":"eda/#libraries","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl import pingouin as pg import math import warnings from src.plotting.plotting import bar_plot , ecdf_plot , boxplot_plot # supressing warnings warnings . filterwarnings ( \"ignore\" ) mpl . rcParams [ \"figure.dpi\" ] = 120 sns . set_style ( \"whitegrid\" ) sns . set_palette ( \"deep\" )","title":"Libraries"},{"location":"eda/#our-dataset","text":"# data fraud_detect_df = pd . read_csv ( \"data/fraud_detection_dataset.csv\" ) fraud_detect_df . columns = [ \"step\" , \"type\" , \"amount\" , \"name_orig\" , \"old_balance_orig\" , \"new_balance_orig\" , \"name_dest\" , \"old_balance_dest\" , \"new_balance_dest\" , \"is_fraud\" , \"is_flagged_fraud\" , ] # change the order, last columns with the target fraud_detect_df = fraud_detect_df [ [ \"step\" , \"type\" , \"amount\" , \"name_orig\" , \"old_balance_orig\" , \"new_balance_orig\" , \"name_dest\" , \"old_balance_dest\" , \"new_balance_dest\" , \"is_flagged_fraud\" , \"is_fraud\" , ] ]","title":"Our Dataset"},{"location":"eda/#how-unbalanced-is-our-data","text":"balance_df = fraud_detect_df [ \"is_fraud\" ] . value_counts ( normalize = True ) . reset_index () balance_df [ \"index\" ] = balance_df [ \"index\" ] . replace ({ 0 : \"not fraud\" , 1 : \"fraud\" }) balance_df [ \"is_fraud\" ] = round ( balance_df [ \"is_fraud\" ], 4 ) # plot bar_plot ( balance_df , x = \"is_fraud\" , y = \"index\" , xlabel = \"Proportion\" , ylabel = \"\" )","title":"How unbalanced is our data?"},{"location":"eda/#how-is_fraud-is-distributed-on-our-categorical-variable-type","text":"type_by_fraud_df = fraud_detect_df [[ \"type\" , \"is_fraud\" ]] . value_counts ( normalize = True ) . reset_index () type_by_fraud_df . columns = [ \"type\" , \"is_fraud\" , \"prop\" ] type_by_fraud_df [ \"is_fraud\" ] = type_by_fraud_df [ \"is_fraud\" ] . replace ({ 0 : \"not fraud\" , 1 : \"fraud\" }) type_by_fraud_df [ \"prop\" ] = round ( type_by_fraud_df [ \"prop\" ], 4 ) bar_plot ( type_by_fraud_df , x = \"prop\" , y = \"type\" , xlabel = \"Proportion\" , ylabel = \"\" , hue = \"is_fraud\" ) Let's perform a Chi2 test, and check if the correlation with our target ( is_fraud ) is significant (our alternative hypothesis). observed , expected , stats = pg . chi2_independence ( fraud_detect_df , x = \"type\" , y = \"is_fraud\" ) stats . round ( 4 ) As wee see, this is a nice variable to add to our model, very low p-values on different tests.","title":"How is_fraud is distributed on our categorical variable type?"},{"location":"eda/#what-about-the-variable-is_flagged_fraud","text":"is_ff_df = fraud_detect_df [ \"is_flagged_fraud\" ] . value_counts () . reset_index () is_ff_df . columns = [ \"is_fraud\" , \"count\" ] fraud_detect_df . drop ([ \"is_flagged_fraud\" ], axis = 1 , inplace = True ) is_fraud count 0 6.362.604 1 16 This variable is not representative, for now wi'll just drop it.","title":"What about the variable is_flagged_fraud?"},{"location":"eda/#lets-look-at-the-amount-variable","text":"We'll be using the ECDF plot here because the size of the dataset is too large, and because of the scale, is really difficult to see, in a meaningful way, the distribution. The ECDF plot will bring on y-axis the percentiles, and x-axis your values for the numeric variable. ecdf_plot ( data = fraud_detect_df , x = \"amount\" , xlabel = \"Amount\" , ylabel = \"Density\" , hue = \"is_fraud\" ) Both of the distributions are skewed to the left, and in 60% of the time will be very difficult to distinguish if is fraud or not, based on those values. So, lets bring those variables to a more normal distribution applying the log transformation. # applying log transformation on this variable def log_transform ( x ): log_transformed = np . log ( x + 1 ) return log_transformed fraud_detect_df [ \"amount_log\" ] = fraud_detect_df [ \"amount\" ] . apply ( log_transform ) fraud_detect_df # let's see for the amount after the transformation ecdf_plot ( data = fraud_detect_df , x = \"amount_log\" , xlabel = \"Amount\" , ylabel = \"Density\" , hue = \"is_fraud\" ) Now, in most of the cases we can clearly see that exist difference between fraud and not fraud based on log transformed values of the amount category. Observing those values as a boxplot, we can see the difference on the medians and infer that those distributions are really different. _ = sns . boxplot ( x = \"is_fraud\" , y = \"amount_log\" , data = fraud_detect_df ) plt . show () So, seems reasonable that we use amount_log for the the rest of the analysis. # droping amount variable fraud_detect_df . drop ([ \"amount\" ], axis = 1 , inplace = True ) fraud_detect_df # let's check this variable related to the type of the transaction boxplot_plot ( data = fraud_detect_df , x = \"type\" , y = \"amount_log\" , hue = \"is_fraud\" , xlabel = \"\" , ylabel = \"Amount (log scale)\" , ) In most of the cases we can see a clear distinction between type , amount and is_fraud . But, for TRANSFER class, the distributions are very similar. Maybe if we remove some lower outliers we could improve the distinction between those distributions, for now, we'll continue without handling outliers.","title":"Let's look at the amount variable"},{"location":"eda/#how-the-hour-of-the-transaction-is-related-to-is_fraud","text":"The step variable is the hour of the transaction . fraud_on_hours_summary_df = ( fraud_detect_df . groupby ([ \"step\" ])[ \"is_fraud\" ] . agg ( count_fraud = sum , mean_fraud = np . mean , median_fraud = np . median , count_values = np . size , ) . reset_index () ) # line plot fig_dims = ( 27 , 4 ) fig , ax = plt . subplots ( figsize = fig_dims ) _ = sns . lineplot ( x = \"step\" , y = \"mean_fraud\" , data = fraud_on_hours_summary_df , ax = ax ) plt . show () This pattern is interesting because seems that there is a threshold on 400hr region, where we have really different kind of distributions. So, I checked what is happing before and after this period regarding to is_fraud variable. is_fraud count before_400hr count after_400hr 0 5.782K 571K 1 4.4K 3.7K We're seeing that we have less data on the second block of the plot, especially for non-fraud. Otherwise, the quantity of fraud stay very similar on both blocks. So, the percentage of fraud on the first 400 hours is 0.0773% and on the second is 0.649% We can also create another variable regarding the day of the month that those ours represent, and perform our analyse on both variables. def get_day ( x : int ): division = x / 24 dom = math . ceil ( division ) return dom fraud_detect_df [ \"day_of_month\" ] = fraud_detect_df [ \"step\" ] . apply ( get_day ) Now, we'll see how this variable is related to is_fraud and the amount_log , but also we'll look at the boxplot for each of the classes on type that have fraud on the observation. df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"TRANSFER\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'TRANSFER'\" , horizontal = True ) df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"CASH_OUT\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'CASH_OUT'\" , horizontal = True ) Was observed that the amount variable , is related just to the difference between origin and not the destination. So, let's add this variable to our dataset. fraud_detect_df [ \"amount_dest\" ] = abs (( fraud_detect_df [ \"old_balance_dest\" ]) - ( fraud_detect_df [ \"new_balance_dest\" ])) fraud_detect_df [ \"amount_dest_log\" ] = fraud_detect_df [ \"amount_dest\" ] . apply ( log_transform ) And also, investigate the those boxplots. df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"TRANSFER\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_dest_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'TRANSFER'\" , horizontal = True ) df_to_plot = fraud_detect_df [ fraud_detect_df [ \"type\" ] == \"CASH_OUT\" ] boxplot_plot ( x = \"day_of_month\" , y = \"amount_log\" , data = df_to_plot , hue = \"is_fraud\" , xlabel = \"Day of the month\" , ylabel = \"Amount (log scale)\" , title = \"Regarding type == 'CASH_OUT'\" , horizontal = True ) ## Exporting our final dataset for modeling final_fraud_df = fraud_detect_df [[ \"day_of_month\" , \"type\" , \"amount_log\" , \"amount_dest_log\" , \"is_fraud\" ]] final_fraud_df . to_csv ( \"data/second-eda-output.csv\" , index = False )","title":"How the hour of the transaction is related to is_fraud?"},{"location":"eda/#the-final-dataset","text":"","title":"The Final Dataset"},{"location":"metrics/","text":"After the training and prediction steps, all the metrics will be collected and added on this report. Perfomance Table Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time (s) baseline 0.964 0.082 0.997 0.541 0.153 26.77 model-v1 0.843 0.446 0.999 0.722 0.584 99.96 model-v2 0.669 0.569 0.999 0.784 0.615 98.99 model-v3 0.027 0.877 0.958 0.918 0.052 98.94 model-v4 0.696 0.676 0.999 0.838 0.686 252.80 Best Model I choose to pick the model with the best value of F1 score . This model could be more balanced between all others trained, and for not overfitting during the training. This model got right 67.6% of the frauds that actually is fraud. And, from all predictions on fraud, 69.6% was right . I believe that this result will not dethrone American Express, but now, I can work on improve those models with another techniques of balancing and hyperparameter tuning to increase the F1 score.","title":"Metrics"},{"location":"metrics/#perfomance-table","text":"Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time (s) baseline 0.964 0.082 0.997 0.541 0.153 26.77 model-v1 0.843 0.446 0.999 0.722 0.584 99.96 model-v2 0.669 0.569 0.999 0.784 0.615 98.99 model-v3 0.027 0.877 0.958 0.918 0.052 98.94 model-v4 0.696 0.676 0.999 0.838 0.686 252.80","title":"Perfomance Table"},{"location":"metrics/#best-model","text":"I choose to pick the model with the best value of F1 score . This model could be more balanced between all others trained, and for not overfitting during the training. This model got right 67.6% of the frauds that actually is fraud. And, from all predictions on fraud, 69.6% was right . I believe that this result will not dethrone American Express, but now, I can work on improve those models with another techniques of balancing and hyperparameter tuning to increase the F1 score.","title":"Best Model"},{"location":"model_v1/","text":"These was the steps of the model-v1 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. The data was not balanced by any technique and the model was not tunned as well. File name: 19_01_22_lr_v1.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics]; Confusion Matrix","title":"Model v1"},{"location":"model_v1/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics];","title":"Pipeline"},{"location":"model_v1/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v2_v3/","text":"These was the steps of the model-v2 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. On the logistic regression was applied the class_weight parameter, with the following values: class_weight : \"balanced\" and {0: 0.10, 1: 0.90} Those models are 21_01_22_lr_w_v1.sav for dict-like params, and 21_01_22_lr_w_v2.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] -.-> F{tune class_weight}:::tune; F{Tune class_weight} -.-> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96; Confusion Matrix V2 V3","title":"Models (v2 and v3)"},{"location":"model_v2_v3/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] -.-> F{tune class_weight}:::tune; F{Tune class_weight} -.-> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96;","title":"Pipeline"},{"location":"model_v2_v3/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v2_v3/#v2","text":"","title":"V2"},{"location":"model_v2_v3/#v3","text":"","title":"V3"},{"location":"model_v4/","text":"These was the steps of the model-v4 using Random Forest , I also used feature engineering and numerical and categorical transformations. The model is 21_01_22_lr_w_v3.sav , and can be found on the respective directory. Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Random Forest]; E --> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96; Confusion Matrix Feature Importance","title":"Model v4"},{"location":"model_v4/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Random Forest]; E --> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96;","title":"Pipeline"},{"location":"model_v4/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v4/#feature-importance","text":"","title":"Feature Importance"}]}