{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The objective of this report is to register the overall steps of every model trained during the challenge of Fraud Detection offer by the Tera and NVidia. Models All pickle files with the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Home"},{"location":"#models","text":"All pickle files with the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Models"},{"location":"baseline/","text":"These was the steps of my baseline using Logistic Regression and very simple transformations. The data was not balanced by any technique and the model was not tunned as well. File name: lrc_baseline.sav Pipeline graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics]; Confusion Matrix","title":"Baseline"},{"location":"baseline/#pipeline","text":"graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics];","title":"Pipeline"},{"location":"baseline/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"metrics/","text":"After the training and prediction steps, all the metrics will be collected and added on this report. Perfomance Table Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time (s) baseline 0.964 0.082 0.997 0.541 0.153 26.77 model-v1 0.843 0.446 0.999 0.722 0.584 99.96 model-v2 0.669 0.569 0.999 0.784 0.615 98.99 model-v3 0.027 0.877 0.958 0.918 0.052 98.94","title":"Metrics"},{"location":"metrics/#perfomance-table","text":"Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time (s) baseline 0.964 0.082 0.997 0.541 0.153 26.77 model-v1 0.843 0.446 0.999 0.722 0.584 99.96 model-v2 0.669 0.569 0.999 0.784 0.615 98.99 model-v3 0.027 0.877 0.958 0.918 0.052 98.94","title":"Perfomance Table"},{"location":"model_v1/","text":"These was the steps of the model-v1 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. The data was not balanced by any technique and the model was not tunned as well. File name: 19_01_22_lr_v1.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics]; Confusion Matrix","title":"Model v1"},{"location":"model_v1/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics];","title":"Pipeline"},{"location":"model_v1/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v2_v3/","text":"These was the steps of the model-v2 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. On the logistic regression was applied the class_weight parameter, with the following values: class_weight : \"balanced\" and {0: 0.10, 1: 0.90} Those models are 21_01_22_lr_w_v1.sav for dict-like params, and 21_01_22_lr_w_v2.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] -.-> F{tune class_weight}:::tune; F{Tune class_weight} -.-> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96; Confusion Matrix V2 V3","title":"Models (v2 and v3)"},{"location":"model_v2_v3/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] -.-> F{tune class_weight}:::tune; F{Tune class_weight} -.-> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96;","title":"Pipeline"},{"location":"model_v2_v3/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v2_v3/#v2","text":"","title":"V2"},{"location":"model_v2_v3/#v3","text":"","title":"V3"},{"location":"model_v4/","text":"These was the steps of the model-v4 using Random Forest , I also used feature engineering and numerical and categorical transformations. The model is 21_01_22_lr_w_v3.sav , and can be found on the respective directory. Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Random Forest]; E --> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96; Confusion Matrix","title":"Model v4"},{"location":"model_v4/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Random Forest]; E --> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96;","title":"Pipeline"},{"location":"model_v4/#confusion-matrix","text":"","title":"Confusion Matrix"}]}