{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The objective of this documentation is to provide an overview about the project. Here, you'll find all the steps of a Data Science Project: Exploratory Data Analysis Iterative Modeling Metrics Analysis API Development Deploy on Heroku The motivation of this project came from the Fraud Detection Challenge offer by the Tera and NVidia. The Challenge Today, to ensure the survival of your business, the great challenge is to reduce operating costs: fraud, especially those related to commercial and financial operations, which impact, in practice, all types of industries and sectors of the economy. From the government, to the bank, from the retailer to the micro-entrepreneur, everyone needs to know and face this challenge. The ACFE report ( Association of Certified Fraud Examiners ) corroborates this perception by pointing to a forecast of 60% growth in the next two years in investments in anti-fraud, a clear example of the challenge organizations are facing. However, the same report points out that 58% of companies declare that they do not have sufficient levels, resources and professionals to act in anti-fraud actions (see table below:) Considering that the goal of fraudsters in general is to have monetary benefits, it makes it evident that the financial sector is one of their main targets. Even the growing investment in preventive and monitoring actions has not been enough to stop or stop the escalation of criminals. According to Psafe , from January to August of last year there were 920 thousand cases in Brazil alone and every minute, 3.6 frauds happen in the country . For example, more than 11 million bank fishing attempts were detected. The financial industry has great representation in Brazil and in the world. To get an idea, the assets of banks in Brazil total R$ 7.4 trillion, exceeding the country's own GDP Infomoney . According to ABECS ( Brazilian Association of Credit Card Companies and Services ), BRL 558 billion was handled in the first quarter of 2021 and credit cards represented BRL $335 billion of that total. There were 6.5 billion transactions , an increase of 11.8%, with emphasis on the debit card, which showed an increase of 163% (see tables below). Despite the opulence of these numbers, the impact of fraud is equally glaring. Cybercrime, considering only credit card transactions, already projected in 2018 an impact of 6 trillion dollars of lost revenue by 2021 around the world. No wonder, in the November 2020 report is listed among the financial sector's priorities to fraud reduction and the use of an artificial intelligence system to monitor it. According to Febraban ( Brazilian Federation of Banks ) to mitigate this risk, an annual expenditure of around BRL 2 billion on IT per year is expected in Brazil. However, the risk is heightened not only by the sophistication of cyber criminals, but also by consumer dissatisfaction that not only abandons the customer base, but also uses the consumer code itself, which guarantees double compensation of the amounts charged. All this, without considering the lawsuits for moral and material damages that can be filed by the consumer. According to the National Council of Justice (CNJ) , the projection is that 28% of the lawsuits in progress have financial institutions as defendants. In this scenario, artificial intelligence emerges as a tool that gives more robustness, agility and flexibility to combat fraud, working 24 hours a day, 7 days a week, making it a more than possible way, necessary for financial institutions to can effectively combat fraudsters. An example of this is American Express, with more than 115 million active credit cards, which fights fraud using inference and Deep Learning, reducing fraud-related expenses by US$ 1.2 trillion , in detection processes that happen in milliseconds . This fact guarantees the company the lowest fraud rate in the world for 13 consecutive years. The Mission You , as a manager, have the mission in this challenge to dethrone American Express as the best institution in the fight against fraud. To do so, you need to propose a solution for fraud detection and analysis that can reduce the company's risks and ensure healthy margins. Remember, the result of your work will give you and your area even more visibility. Be judicious, use good arguments, facts and justifications for your proposal, as well as, of course, make a good execution of your project. The mission is not easy, as the large volume of information and technological legacy are the reality of most corporations in the sector, even among leading companies. In addition, artificial intelligence initiatives demand great capacity Models All models are serialized as pickle files to reproducibility. All the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Home"},{"location":"#the-challenge","text":"Today, to ensure the survival of your business, the great challenge is to reduce operating costs: fraud, especially those related to commercial and financial operations, which impact, in practice, all types of industries and sectors of the economy. From the government, to the bank, from the retailer to the micro-entrepreneur, everyone needs to know and face this challenge. The ACFE report ( Association of Certified Fraud Examiners ) corroborates this perception by pointing to a forecast of 60% growth in the next two years in investments in anti-fraud, a clear example of the challenge organizations are facing. However, the same report points out that 58% of companies declare that they do not have sufficient levels, resources and professionals to act in anti-fraud actions (see table below:) Considering that the goal of fraudsters in general is to have monetary benefits, it makes it evident that the financial sector is one of their main targets. Even the growing investment in preventive and monitoring actions has not been enough to stop or stop the escalation of criminals. According to Psafe , from January to August of last year there were 920 thousand cases in Brazil alone and every minute, 3.6 frauds happen in the country . For example, more than 11 million bank fishing attempts were detected. The financial industry has great representation in Brazil and in the world. To get an idea, the assets of banks in Brazil total R$ 7.4 trillion, exceeding the country's own GDP Infomoney . According to ABECS ( Brazilian Association of Credit Card Companies and Services ), BRL 558 billion was handled in the first quarter of 2021 and credit cards represented BRL $335 billion of that total. There were 6.5 billion transactions , an increase of 11.8%, with emphasis on the debit card, which showed an increase of 163% (see tables below). Despite the opulence of these numbers, the impact of fraud is equally glaring. Cybercrime, considering only credit card transactions, already projected in 2018 an impact of 6 trillion dollars of lost revenue by 2021 around the world. No wonder, in the November 2020 report is listed among the financial sector's priorities to fraud reduction and the use of an artificial intelligence system to monitor it. According to Febraban ( Brazilian Federation of Banks ) to mitigate this risk, an annual expenditure of around BRL 2 billion on IT per year is expected in Brazil. However, the risk is heightened not only by the sophistication of cyber criminals, but also by consumer dissatisfaction that not only abandons the customer base, but also uses the consumer code itself, which guarantees double compensation of the amounts charged. All this, without considering the lawsuits for moral and material damages that can be filed by the consumer. According to the National Council of Justice (CNJ) , the projection is that 28% of the lawsuits in progress have financial institutions as defendants. In this scenario, artificial intelligence emerges as a tool that gives more robustness, agility and flexibility to combat fraud, working 24 hours a day, 7 days a week, making it a more than possible way, necessary for financial institutions to can effectively combat fraudsters. An example of this is American Express, with more than 115 million active credit cards, which fights fraud using inference and Deep Learning, reducing fraud-related expenses by US$ 1.2 trillion , in detection processes that happen in milliseconds . This fact guarantees the company the lowest fraud rate in the world for 13 consecutive years.","title":"The Challenge"},{"location":"#the-mission","text":"You , as a manager, have the mission in this challenge to dethrone American Express as the best institution in the fight against fraud. To do so, you need to propose a solution for fraud detection and analysis that can reduce the company's risks and ensure healthy margins. Remember, the result of your work will give you and your area even more visibility. Be judicious, use good arguments, facts and justifications for your proposal, as well as, of course, make a good execution of your project. The mission is not easy, as the large volume of information and technological legacy are the reality of most corporations in the sector, even among leading companies. In addition, artificial intelligence initiatives demand great capacity","title":"The Mission"},{"location":"#models","text":"All models are serialized as pickle files to reproducibility. All the models can be find on models directory, accessing the github of the project . I'm using the following convention for name the pickle files: {date_start_training}{model}v_{ordinal_number_qtt_trained_that_date}.sav","title":"Models"},{"location":"baseline/","text":"These was the steps of my baseline using Logistic Regression and very simple transformations. The data was not balanced by any technique and the model was not tunned as well. File name: lrc_baseline.sav Pipeline graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics]; Confusion Matrix","title":"Baseline"},{"location":"baseline/#pipeline","text":"graph TD A[Raw data: fraud_detection_dataset.csv] --> B[Drop Columns]; B[Drop Columns] --> C[Filter classes on type]; C[Filter classes on type] --> D[Train Test Split]; D[Train Test Split] --> E[Label Encoding on type]; E[Label Encoding on type] --> F[Scaling]; F[Scaling] --> G[Fit Logistic Regression]; G[Fit Logistic Regression] --> H[Predict]; H[Predict] --> I[Calculate Metrics];","title":"Pipeline"},{"location":"baseline/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"metrics/","text":"After the training and prediction steps, all the metrics will be collected and added on this report. Perfomance Table Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time (s) baseline 0.964 0.082 0.997 0.541 0.153 26.77 model-v1 0.843 0.446 0.999 0.722 0.584 99.96 model-v2 0.669 0.569 0.999 0.784 0.615 98.99 model-v3 0.027 0.877 0.958 0.918 0.052 98.94 model-v4 0.696 0.676 0.999 0.838 0.686 252.80","title":"Metrics"},{"location":"metrics/#perfomance-table","text":"Keep tracking of your experiments is a very important task of the Data Scientist, so check the following table to have all the information about the models trained during the challenge of Fraud Detection. Model Precision Recall Accuracy AUC F1 Time (s) baseline 0.964 0.082 0.997 0.541 0.153 26.77 model-v1 0.843 0.446 0.999 0.722 0.584 99.96 model-v2 0.669 0.569 0.999 0.784 0.615 98.99 model-v3 0.027 0.877 0.958 0.918 0.052 98.94 model-v4 0.696 0.676 0.999 0.838 0.686 252.80","title":"Perfomance Table"},{"location":"model_v1/","text":"These was the steps of the model-v1 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. The data was not balanced by any technique and the model was not tunned as well. File name: 19_01_22_lr_v1.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics]; Confusion Matrix","title":"Model v1"},{"location":"model_v1/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] --> F[Predict]; F[Predict] --> G[Calculate Metrics];","title":"Pipeline"},{"location":"model_v1/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v2_v3/","text":"These was the steps of the model-v2 using Logistic Regression , I also used feature engineering and numerical and categorical transformations. On the logistic regression was applied the class_weight parameter, with the following values: class_weight : \"balanced\" and {0: 0.10, 1: 0.90} Those models are 21_01_22_lr_w_v1.sav for dict-like params, and 21_01_22_lr_w_v2.sav Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] -.-> F{tune class_weight}:::tune; F{Tune class_weight} -.-> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96; Confusion Matrix V2 V3","title":"Models (v2 and v3)"},{"location":"model_v2_v3/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Logistic Regression]; E[Fit Logistic Regression] -.-> F{tune class_weight}:::tune; F{Tune class_weight} -.-> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96;","title":"Pipeline"},{"location":"model_v2_v3/#confusion-matrix","text":"","title":"Confusion Matrix"},{"location":"model_v2_v3/#v2","text":"","title":"V2"},{"location":"model_v2_v3/#v3","text":"","title":"V3"},{"location":"model_v4/","text":"These was the steps of the model-v4 using Random Forest , I also used feature engineering and numerical and categorical transformations. The model is 21_01_22_lr_w_v3.sav , and can be found on the respective directory. Pipeline graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Random Forest]; E --> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96; Confusion Matrix","title":"Model v4"},{"location":"model_v4/#pipeline","text":"graph TD A[Transformed Data: second-eda-output.csv] --> B[Type conversion]; B[Type conversion] --> C[Train Test Split]; C[Train Test Split] --> D[One Hot Encoder on all categorical variables]; D[One Hot Encoder] --> E[Fit Random Forest]; E --> G[Predict]; G[Predict] --> H[Calculate Metrics]; classDef tune fill:#f96;","title":"Pipeline"},{"location":"model_v4/#confusion-matrix","text":"","title":"Confusion Matrix"}]}